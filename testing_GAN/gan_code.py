# -*- coding: utf-8 -*-
"""Code for GradLab

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16lkSrBjZmOBCU_zc1yaEiIzKHowa8hhJ

This mounts google-drive on colab
"""

"""
from google.colab import drive
drive.mount('/content/drive')

"""

"""Importing the Libraries"""

import numpy      as np
import tensorflow as tf
import os
import cv2
import matplotlib.pyplot as plt
import PIL.Image
from keras               import layers, Input, Model
from PIL                 import Image
import pathlib
#from google.colab.patches import cv2_imshow

"""The directory for the datasets"""

Trainingset_directory = '/cis/phd/ki6989/Datasetz'
#Testingsets_directory = '/content/drive/MyDrive/chest_xray/chest_xray/test'
#Validationset_directory = '/content/drive/MyDrive/chest_xray/chest_xray/val'

# You can use either the Testing and Validation directory 
# to see the normal and Pneumonia

os.listdir(Trainingset_directory)
train_n = Trainingset_directory + '/Normal/'
train_p = Trainingset_directory + '/Pneumonia/'

#norm1 = os.listdir(train_n)[:]
#sick1 = os.listdir(train_p)[:]

#r = [train_n+norm for norm in norm1]

"""Test for more 5000 samples"""

# Since GAN does not require train, test or validation set
# Adding all those sets of data will help the GAN in reconstructing synthetic samples
# The trainingset directory has most of the data counting up to 5200.

norm_pic = os.listdir(train_n)[:]
sick_pic = os.listdir(train_p)[:]

norm_loc = [train_n+norm for norm in norm_pic]
sick_loc = [train_p+norm for norm in sick_pic]

Datasets = np.concatenate([norm_loc, sick_loc])

#Let's plt these images
Normal = PIL.Image.open(Datasets[0])
Sick   = PIL.Image.open(Datasets[4000])

f  = plt.figure(figsize= (10,6))
a1 = f.add_subplot(1,2,1)
img_plot = plt.imshow(Normal)
a1.set_title('Normal')

a2 = f.add_subplot(1, 2, 2)
img_plot = plt.imshow(Sick)
a2.set_title('Pneumonia')

"""r = cv2.imread(Datasets[1])
r.shape

Resizing the dimension of the images
"""

# I couldn't change all the images shape. I dont know why
# I had to reduce it to a  1000

Data_resized = []

for i in range(0, 5000):
  Data = cv2.imread(Datasets[i], cv2.IMREAD_UNCHANGED)
  Data = cv2.cvtColor(Data, cv2.COLOR_BGR2RGB)

  width = 256 
  height = 256
  dim = (width, height)

  Data_r = cv2.resize(Data, dim, interpolation=cv2.INTER_AREA)
  Data_resized.append(Data_r)

plt.imshow(Data_resized[1])

Data_resized[0].shape

"""The GAN MODEL"""

class GAN:
  """
  This class houses the Generator CNN and the Discriminator CNN.
  """

  @staticmethod
  def Generator(input_shape=100, channels = 3):
    """
    This method computes the Generator Convolutional Neural Network.
    This code uses the Functional API provided in tensorflow.org
    input_shape: is the randomly generated vector from the normal distribution.
    channel: the number of target channels (ie 3 for RGB)
    """
    Input_gen = keras.Input(shape=(input_shape,))
    
    x = layers.Dense(64*64*128)(Input_gen)
    x = layers.Reshape((64, 64, 128))(x)
    
    x = layers.Conv2DTranspose(128, kernel_size=(5,5), strides=2, padding='same')(x)
    x = layers.Activation('relu')(x)
    x = layers.BatchNormalization()(x)
    
    x = layers.Conv2DTranspose(64, kernel_size=(5,5), strides=2, padding='same')(x)
    x = layers.Activation('relu')(x)
    x = layers.BatchNormalization()(x)
    
    x = layers.Conv2DTranspose(32, kernel_size=(5,5), strides=1, padding='same')(x)
    x = layers.Activation('relu')(x)
    x = layers.BatchNormalization()(x)
    
    x = layers.Conv2DTranspose(3, kernel_size=(5,5), strides=1, padding='same')(x)
    x = layers.Activation('relu')(x)
    x = layers.BatchNormalization()(x)
    
    #The output layer using a sigmoid activation
    x = layers.Activation('sigmoid')(x)
    generator = Model(Input_gen, x, name='generator')
    return generator

  @staticmethod
  def Discriminator(input_shape=[256,256,3]):
    Input_dis = keras.Input(input_shape)
    x = layers.LeakyReLU(alpha=0.2)(Input_dis)
    x = layers.Conv2D(32, kernel_size=[5,5], strides=2, padding='same')(x)
    
    x = layers.LeakyReLU(alpha=0.2)(x)
    x = layers.Conv2D(64, kernel_size=[5,5], strides=2, padding='same')(x)
    
    x = layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Conv2D(128, kernel_size=[5,5], strides=2, padding='same')(x)
    
    x = layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Conv2D(256, kernel_size=[5,5], strides=1, padding='same')(x)
    
    #The output layer using a sigmoid activation
    x = layers.Flatten()(x)
    x = layers.Dense(1, activation='sigmoid')(x)
    
    #Build Model
    discriminator = keras.Model(Input_dis, x, name='discriminator')
    return discriminator

"""Testing the Generator with a random noise"""

generator = GAN.Generator()

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)

plt.imshow(generated_image[0, :, :, 0], cmap='gray')

discriminator = GAN.Discriminator()
decision = discriminator(generated_image)
print (decision)

"""The Adam Optimizer"""

# This custom Optimizer created follows the procedures
# provided by keras to write an optimizer of the user's
# choice. 

class MyAdamOptimizer(keras.optimizers.Optimizer):
  def __init__(self, learning_rate=0.001, beta_1=0.9,
               beta_2=0.999, epsilon=1e-7, amsgrad=False,
               name='MyAdamOptimizer',**kwargs):
    super().__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr',learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon
    self.amsgrad = amsgrad

  #Create slots for the variables
  def _create_slots(self, var_list):
    for var in var_list:
      self.add_slot(var, "m")
    for var in var_list:
      self.add_slot(var, "v")
    if self.amsgrad:
      for var in var_list:
        self.add_slot(var, "vhat")

  # Weights generation
  # Took the weights set by Keras
  def set_weights(self, weights):
    params = self.weights
    num_vars = int((len(params)-1)/2)
    if len(weights) == 3*num_vars + 1:
      weights = weights[: len(params)]
      super().set_weights(weights)
  
  # The math starts from here, in tensorflow way. I couldn't use numpy here ...
  @tf.function
  def _resource_apply_dense(self, grad, var):
    var_dtype = var.dtype.base_dtype

    # Define the hyperparamter 
    epsilon  = tf.convert_to_tensor(self.epsilon, var_dtype)
    beta_1_t = self._get_hyper("beta_1",var_dtype)
    beta_2_t = self._get_hyper("beta_2",var_dtype)

    lr = self._decayed_lr(var_dtype)
    m  = self.get_slot(var, "m")
    v  = self.get_slot(var, "v")

    #First moment m_t = beta1 * m + (1 - beta1) * g_t
    m_t1 = grad * (1.0 - beta_1_t)
    m_t  = tf.compat.v1.assign(m, (m * beta_1_t) + m_t1)

    #Second moment v_t = beta2 * v + (1 - beta2) * (g_t * g_t)
    v_t1 = (grad * grad) * (1-beta_2_t)
    v_t  = tf.compat.v1.assign(v, (v * beta_2_t) + v_t1)


    # Update
    if not self.amsgrad:
      v_sqrt = tf.sqrt(v_t)
      delta_vhat = tf.compat.v1.assign_sub(var, lr * m_t/((v_sqrt) + epsilon))
    else:
      vhat = self.get_slot(var, "vhat")
      v_hat = tf.compat.v1.assign(vhat, tf.maximum(vhat, v_t))
      v_hat_sqrt = tf.sqrt(v_hat)
      delta_vhat = tf.compat.v1.assign_sub(var, lr * m_t/((v_hat_sqrt) + epsilon))

  def get_config(self):
    base_config = super().get_config()
    return {
        **base_config,
        "learning_rate": self._serialize_hyperparameter("learning_rate"),
        "decay": self._initial_decay,
        "beta_1": self._serialize_hyperparameter("beta_1"),
        "beta_2": self._serialize_hyperparameter("beta_2"),
        "epsilon": self.epsilon,
        "amsgrad": self.amsgrad,
        }

#Build the Adversial Network 

def AdversarialNet():
  noise = 100     
  lr = 1E-4
  decay_rate = 2.5E-4

  D_Gen = GAN.Generator(noise, 3)
  
  #Define the Discriminator 
  D_Dis = GAN.Discriminator((256,256,3,))

  # The Optimizer for the Discriminator
  D_DisOpt = MyAdamOptimizer(learning_rate=lr, beta_1=0.5, decay=decay_rate)

  #Compiling......
  D_Dis.compile(optimizer=D_DisOpt, 
                loss='binary_crossentropy')
  
  #We are freezing the weights of the discriminator
  D_Dis.trainable = False

  Gan_Input   = Input(shape=(100,))
  Gan_Output  = D_Dis(D_Gen(Gan_Input))
  Adversarial = Model(Gan_Input, Gan_Output)

  #Optimizer
  Adversarial_Opt = MyAdamOptimizer(learning_rate=lr, beta_1=0.5, decay=decay_rate)
  Adversarial.compile(optimizer=Adversarial_Opt, loss='binary_crossentropy')

  return D_Gen, D_Dis, Adversarial

def plot_images(fake_images, step):
    
    plt.figure(figsize=(10,7))
    num_images = fake_images.shape[0]
    
    image_size = fake_images.shape[1]
    #rows = int(np.sqrt(fake_images.shape[0]))
    
    for i in range(num_images):
        plt.subplot(1, 2, i + 1)
        image = np.reshape(fake_images[i], [image_size, image_size, 3])
        plt.imshow(image)
        plt.axis('off')
    plt.savefig('Image_epoch500_GANlab3{:04d}.png'.format(step))

"""TRAINING THE NETWORKS"""

def Training(D_Gen, D_Dis, Adversarial, noise=100, image_size = 256):
  batch_size = 32
  epoch = 20000

  Train_values = np.reshape(Data_resized, [-1, image_size, image_size, 3])
  
  #Standardizing the Training sets
  Train_sets = Train_values.astype('float32') / 255
    
  #Input for testing generator 
  Test_input = np.random.uniform(-1.0, 1.0, size=[2, noise])
    
  #Start training
  for i in range(epoch):

    #TRAINING THE DISCRIMATOR
        
    # Get fake images from Generator
    noise_input = np.random.uniform(-1.0,1.0, size=[batch_size, noise])
    fake_images = D_Gen.predict(noise_input)
        
    # Get real images from training set
    img_indexes = np.random.randint(0, Train_sets.shape[0], size=batch_size)
    real_images = Train_sets[img_indexes]
        
    #3. Prepare input for training Discriminator
    X = np.concatenate((real_images, fake_images))
        
    #4. Labels for training
    y_real = np.ones((batch_size, 1))
    y_fake = np.zeros((batch_size, 1))
    y = np.concatenate((y_real, y_fake))
        
    #5. Train Discriminator
    d_loss = D_Dis.train_on_batch(X, y)
        
        
    #Train ADVERSARIAL Network
        
    #1. Prepare input - create a new batch of noise
    X = np.random.uniform(-1.0,1.0, size=[batch_size, noise])
        
    #2. Prepare labels - training Adversarial network to lie :) - All 1s
    y = np.ones((batch_size, 1))
        
    #3. Train - Pls note Discrimator is not getting trained here
    a_loss = Adversarial.train_on_batch(X, y)
        
    if i % 100 == 0:
      #Print loss and Accuracy for both networks
      print("%s [Discriminator loss: %f, Adversarial loss: %f]" % (i, d_loss, a_loss) )
        
      #Save generated images to see how well Generator is doing
    if (i+1) % 500 == 0:
      #Generate images
      fake_images = D_Gen.predict(Test_input)
            
      #Display images
      plot_images(fake_images, i+1)

      #Storing the losses
      #D_losses = [], A_losses = []
      #D_losses.append(d_loss)
      #A_losses.append(a_loss)

D, E, F = AdversarialNet()

Training(D, E, F)

#def plot_losses(G_losses, A_losses):
  #x = np.linspace(0, 10, 20)
  #plt.plot(x, G_losses)
  #plt.plot(x, A_losses)
  #plt.show()
